# ðŸ“˜ Learning Guide: RAG Chatbot with MongoDB & Azure OpenAI

This guide explains the complete workflow of building a Retrieval-Augmented Generation (RAG) chatbot.  
It breaks down each section of the notebook with detailed explanations and **paper sketches** (ASCII diagrams).

---

## ðŸ”¹ Section 1: Knowledge Base

**Goal:** Convert raw text into structured documents.

- Start with long raw text documents.
- Convert each into a LangChain `Document` object:
  - `page_content` â†’ actual text
  - `metadata` â†’ labels (`source`, `doc_id`)

**Paper Sketch:**
```text
[Raw long docs]
    â”œâ”€ Doc 1: "Python is a high-level language..."
    â”œâ”€ Doc 2: "Machine learning is a subset of AI..."
    â”œâ”€ Doc 3: "Web development involves..."
    â””â”€ Doc 4: "Discord bots are applications..."

     â”‚
     â–¼

[LangChain Documents]
    â”œâ”€ Document(page_content="Python...", metadata={"source": "doc_1", "doc_id": 0})
    â”œâ”€ Document(page_content="Machine learning...", metadata={"source": "doc_2", "doc_id": 1})
    â”œâ”€ Document(page_content="Web development...", metadata={"source": "doc_3", "doc_id": 2})
    â””â”€ Document(page_content="Discord bots...", metadata={"source": "doc_4", "doc_id": 3})
ðŸ”¹ Section 2: Chunking & Splitting
Goal: Break down long documents into smaller, overlapping chunks.

Use RecursiveCharacterTextSplitter (chunk_size = 1000, overlap = 50).

Overlap prevents losing boundary info.

Extract chunks_texts for embeddings.

Paper Sketch:

text
Copy code
[LangChain Documents]
     â”‚
     â–¼
RecursiveCharacterTextSplitter
     â”‚
     â–¼
Chunks created:
    â”œâ”€ Chunk 1 (1000 chars)
    â”œâ”€ Chunk 2 (last 50 overlap + 950 new)
    â””â”€ ...
Each chunk keeps:
    page_content = "chunk text"
    metadata = {"source": "doc_X", "doc_id": Y}
ðŸ”¹ Section 3: Embeddings
Goal: Represent chunks as vectors.

Use all-MiniLM-L6-v2 (SentenceTransformer).

Each chunk â†’ 384-dim embedding.

Pair embeddings back with chunk text + metadata.

Paper Sketch:

text
Copy code
[Chunk Texts]
     â”‚
     â–¼
Embedding Model (MiniLM-L6-v2)
     â”‚
     â–¼
doc_embeddings (num_chunks Ã— 384)
     â”œâ”€ [0.12, -0.45, ...] â† Chunk 1
     â”œâ”€ [0.05,  0.99, ...] â† Chunk 2
     â””â”€ [0.77, -0.21, ...] â† Chunk 3

     â”‚
     â–¼
documents_to_insert:
    â”œâ”€ {"_id": 0, "text": "Python...", "embedding": [...], "source": "doc_1"}
    â”œâ”€ {"_id": 1, "text": "ML...", "embedding": [...], "source": "doc_2"}
    â””â”€ {"_id": 2, "text": "Web dev...", "embedding": [...], "source": "doc_3"}
ðŸ”¹ Section 4: MongoDB Atlas Integration
Goal: Store vectors in a database and enable search.

Insert documents_to_insert into MongoDB Atlas.

Create vector index (numDimensions=384, metric=cosine).

Query: embed user query â†’ $vectorSearch â†’ top-k chunks.

Paper Sketch:

text
Copy code
Insert:
[documents_to_insert] â”€â”€> MongoDB Collection

Index:
Index name = vector_index
Path = embedding
numDimensions = 384
similarity = cosine

Query:
User Q = "Who created Python?"
     â”‚
     â–¼
Embed â†’ [vector 384-dim]
     â”‚
     â–¼
$vectorSearch
     â”‚
     â–¼
Top-k docs:
    â”œâ”€ {"_id": 0, "text": "Python created by Guido...", "source": "doc_1", "score": 0.99}
    â””â”€ {"_id": 3, "text": "Python readability...", "source": "doc_1", "score": 0.87}
ðŸ”¹ Section 5: LLM Integration & Prompt Engineering
Goal: Make the LLM answer only from retrieved context.

Configure Azure OpenAI GPT-3.5 Turbo.

Create prompts:

System prompt â†’ rules (only use context, cite sources).

User prompt â†’ insert {context} and {question}.

Build context from docs.

Call Azure GPT via call_azure_chat.

rag_query() = retrieval + context + prompt + LLM â†’ answer.

Paper Sketch:

text
Copy code
User Question: "Who created Python?"
     â”‚
     â–¼
MongoDB Vector Search â†’ Top-k Docs
     â”‚
     â–¼
build_context_from_docs()
     â”‚
     â–¼
Context string:
    [source:doc_1] Python was created by Guido van Rossum in 1991.

Prompt:
SYSTEM_PROMPT = "Answer ONLY using context. Cite sources."
USER_PROMPT =
    CONTEXT:
    [source:doc_1] Python...

    QUESTION:
    Who created Python?

    ANSWER:

     â”‚
     â–¼
Azure GPT-3.5 Turbo
     â”‚
     â–¼
Answer: "Python was created by Guido van Rossum in 1991. [source:doc_1]"
ðŸ”¹ Section 6: Full RAG Pipeline
Goal: Orchestrate everything into one function.

retrieve_docs_for_query() â†’ embed query, search, normalize.

build_rag_prompt() â†’ build context + prompt.

call_llm_for_rag() â†’ send prompt to Azure GPT.

run_rag_pipeline() â†’ full flow, return structured result.

Paper Sketch:

text
Copy code
[User Question]
     â”‚
     â–¼
retrieve_docs_for_query()
     â”‚
     â–¼
Top-k Docs (normalized: _id, text, source, score)
     â”‚
     â–¼
build_rag_prompt()
     â”‚
     â–¼
Prompt = {system, user, context}
     â”‚
     â–¼
call_llm_for_rag()
     â”‚
     â–¼
Azure GPT â†’ Answer
     â”‚
     â–¼
Return:
{
  "question": "Who created Python?",
  "answer": "Python was created by Guido van Rossum in 1991. [source:doc_1]",
  "docs": [...],
  "sources": ["doc_1"],
  "context": "[source:doc_1] Python..."
}
âœ… Final Overview (End-to-End)
Full pipeline in one sketch:

text
Copy code
[Raw Docs] 
     â–¼
LangChain Documents
     â–¼
Chunking & Splitting
     â–¼
Embeddings (MiniLM, 384-dim)
     â–¼
MongoDB Atlas (vector index)
     â–¼
User Query â†’ Embed
     â–¼
MongoDB $vectorSearch â†’ Top-k Docs
     â–¼
Context Builder â†’ Prompt
     â–¼
Azure GPT-3.5 Turbo
     â–¼
Answer (grounded + citations)
