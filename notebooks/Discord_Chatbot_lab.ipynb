{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "THcIMtoYX-Gi"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG) Chatbot with MongoDB & Azure OpenAI\n",
        "\n",
        "This notebook demonstrates the complete workflow for building a **Retrieval-Augmented Generation (RAG) chatbot**.  \n",
        "The system integrates the following components:  \n",
        "\n",
        "- **Knowledge Base Construction** – curating documents and splitting them into chunks.  \n",
        "- **Embeddings** – generating dense vector representations using `SentenceTransformers`.  \n",
        "- **Vector Database** – storing and retrieving embeddings with **MongoDB Atlas Vector Search** (with cosine similarity fallback).  \n",
        "- **LLM Integration** – connecting to **Azure OpenAI GPT-3.5 Turbo** for context-aware responses.  \n",
        "- **RAG Pipeline** – combining retrieval and generation for grounded answers.  \n",
        "- **Evaluation & Testing** – validating chatbot responses on sample and edge queries.  \n",
        "\n",
        "This notebook is structured for clarity, reproducibility, and deployment-readiness, making it a solid reference for **end-to-end RAG implementation**.\n"
      ],
      "metadata": {
        "id": "Q85ZIn4cY976"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package Installation  \n",
        "\n",
        "In this step, we install all the required Python libraries:  \n",
        "\n",
        "- **sentence-transformers** – for generating embeddings of text chunks.  \n",
        "- **numpy & scikit-learn** – for numerical operations and similarity calculations.  \n",
        "- **langchain & langchain-text-splitters** – to handle document chunking and retrieval workflows.   \n",
        "- **pymongo & dnspython** – to connect and interact with **MongoDB Atlas** (vector database).  \n",
        "- **openai** – to integrate with **Azure OpenAI GPT models** for LLM responses.  \n"
      ],
      "metadata": {
        "id": "DOwza-Q5XQ0x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCatv5QpMcDY",
        "outputId": "b9fffddd-9908-45ff-d6db-6a81c5e352ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (0.3.11)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "\n",
        "!pip install sentence-transformers numpy scikit-learn\n",
        "!pip install langchain langchain-text-splitters\n",
        "!pip install pymongo dnspython --quiet\n",
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "c4a0xmSTMsmL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Create a knowledge base\n",
        "\n",
        "In this step, we define a **small custom knowledge base** containing documents on topics like Python, Machine Learning, Web Development, and Discord Bots.  \n",
        "Each document is wrapped into a **LangChain `Document` object** with metadata, making it easier to split, embed, and later retrieve relevant text chunks.  \n",
        "\n",
        "This forms the foundation for our RAG pipeline.  "
      ],
      "metadata": {
        "id": "THcIMtoYX-Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple knowledge base\n",
        "print(\"Setting up knowledge base...\")\n",
        "\n",
        "long_docs = [\n",
        "    \"\"\"Python Programming Language Overview:\n",
        "    Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.\n",
        "    It emphasizes code readability with its notable use of significant whitespace.\n",
        "    Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\n",
        "    The language is widely used for web development, data analysis, artificial intelligence, and automation.\n",
        "    Popular Python frameworks include Django for web development, NumPy for scientific computing,\n",
        "    and TensorFlow for machine learning applications.\"\"\",\n",
        "\n",
        "    \"\"\"Machine Learning and Artificial Intelligence:\n",
        "    Machine learning is a subset of artificial intelligence that enables computers to learn from data automatically.\n",
        "    There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\n",
        "    Supervised learning uses labeled data to train models for prediction tasks.\n",
        "    Unsupervised learning finds patterns in data without labels, such as clustering similar items.\n",
        "    Reinforcement learning trains agents to make decisions through trial and error with rewards and penalties.\n",
        "    Popular machine learning libraries include scikit-learn, TensorFlow, PyTorch, and Keras.\"\"\",\n",
        "\n",
        "    \"\"\"Web Development Technologies:\n",
        "    Web development involves creating websites and web applications using various technologies.\n",
        "    Frontend development focuses on user interfaces using HTML, CSS, and JavaScript.\n",
        "    Backend development handles server-side logic, databases, and APIs using languages like Python, Java, or Node.js.\n",
        "    REST APIs provide a way for different systems to communicate over HTTP using standard methods.\n",
        "    Modern web frameworks like React, Vue.js, and Angular help build interactive user interfaces.\n",
        "    Database systems like PostgreSQL, MongoDB, and Redis store and manage application data efficiently.\"\"\",\n",
        "\n",
        "    \"\"\"Discord Bot Development:\n",
        "    Discord bots are applications that can interact with Discord servers automatically.\n",
        "    They can respond to messages, moderate chat, play music, and perform various automated tasks.\n",
        "    Discord bots are built using Discord's API and can be developed in multiple programming languages.\n",
        "    Python developers often use the discord.py library to create bots with features like slash commands.\n",
        "    Bots require proper authentication using bot tokens and must be invited to servers with appropriate permissions.\n",
        "    Common bot features include welcome messages, role management, music playback, and custom commands.\"\"\"\n",
        "]\n",
        "\n",
        "# Create LangChain Document objects but keep a different variable name to avoid future shadowing\n",
        "from langchain.schema import Document\n",
        "\n",
        "langchain_documents = [\n",
        "    Document(page_content=text, metadata={\"source\": f\"doc_{i+1}\", \"doc_id\": i})\n",
        "    for i, text in enumerate(long_docs)\n",
        "]\n",
        "\n",
        "print(f\"Created knowledge base with {len(langchain_documents)} documents\")\n",
        "\n",
        "# Quick validation prints (helps spot problems immediately)\n",
        "print(\"\\nSample metadata for doc 1:\", langchain_documents[0].metadata)\n",
        "print(\"\\nSample text (first 300 chars):\\n\", langchain_documents[0].page_content[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV8nMb0UNZIr",
        "outputId": "ed02cca5-e99a-427b-b57e-b9103c5d681c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up knowledge base...\n",
            "Created knowledge base with 4 documents\n",
            "\n",
            "Sample metadata for doc 1: {'source': 'doc_1', 'doc_id': 0}\n",
            "\n",
            "Sample text (first 300 chars):\n",
            " Python Programming Language Overview:\n",
            "    Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.\n",
            "    It emphasizes code readability with its notable use of significant whitespace.\n",
            "    Python supports multiple programming paradigms including procedural, object-o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Chunking and splitting\n",
        "\n",
        "n this step, we prepare the knowledge base for embedding by **splitting long documents into smaller, overlapping text chunks**.  \n",
        "\n",
        "- **Why chunking?**  \n",
        "  Large documents are difficult to process directly with embeddings or LLMs. Breaking them into **smaller, context-rich chunks** allows for more precise retrieval and avoids exceeding token limits.  \n",
        "\n",
        "- **How it works:**  \n",
        "  - We use **LangChain’s `RecursiveCharacterTextSplitter`**.  \n",
        "  - Each chunk is set to **1000 characters** with an **overlap of 50 characters**.  \n",
        "  - Overlap ensures that information at the boundary of one chunk is also present in the next, reducing context loss.  \n",
        "  - Chunks are stored as **LangChain `Document` objects** with metadata, then extracted into plain text (`chunks_texts`) for embedding.  \n",
        "\n",
        "This chunking step bridges raw documents and vector embeddings, making retrieval accurate and contextually relevant.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "50zWT7UzQkih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a langchain text splitter\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunk_size = 1000\n",
        "chunk_overlap = 50\n",
        "# separators order matters; keep paragraph and newline splits before sentences/space\n",
        "separators = [\"\\n\\n\", \"\\n\", \". \", \".\", \" \"]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    separators=separators\n",
        ")\n",
        "print(f\"Text splitter created: {chunk_size} chars, {chunk_overlap} overlap\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyYh1O1tO90d",
        "outputId": "9b1bbed8-bb78-4412-cf1e-5af85c4060f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text splitter created: 1000 chars, 50 overlap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert long_docs into LangChain Document objects\n",
        "\n",
        "from langchain.schema import Document\n",
        "\n",
        "if 'langchain_documents' in globals():\n",
        "    documents = langchain_documents\n",
        "else:\n",
        "    documents = [\n",
        "        Document(page_content=doc, metadata={\"source\": f\"documents_{i+1}\", \"doc_id\": i})\n",
        "        for i, doc in enumerate(long_docs)\n",
        "    ]\n",
        "\n",
        "print(f\"Created {len(documents)} documents to langchain format\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_ScQbKfO-21",
        "outputId": "3b119e40-ac0b-48e6-e142-e2c825d9f02f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 4 documents to langchain format\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split all documents into chunks\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)  # pass the list directly\n",
        "print(f\"Created {len(chunks)} total chunks from {len(documents)} original documents\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DljGuQ1kPJz6",
        "outputId": "97cf8641-06b5-4056-96c9-a8c39a02e8bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 4 total chunks from 4 original documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Chunks\n",
        "\n",
        "print(\"Chunking breakdown:\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"--- CHUNK {i+1} ---\")\n",
        "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
        "    print(f\"Source: {chunk.metadata.get('source')}\")\n",
        "    # Print a shortened preview to keep output compact\n",
        "    preview = chunk.page_content[:300].replace(\"\\n\", \" \")\n",
        "    print(f\"Text (preview): '{preview}...'\")\n",
        "\n",
        "    # Show only first 3 chunks in detail to avoid huge output; indicate remaining count\n",
        "    if i >= 2:\n",
        "        remaining = max(0, len(chunks) - (i+1))\n",
        "        print(f\"  ... and {remaining} more chunks\")\n",
        "        break\n",
        "\n",
        "    # Overlap check for chunks 2 and onward\n",
        "    if i > 0:\n",
        "        prev_chunk = chunks[i-1]\n",
        "        current_start = chunk.page_content[:30]\n",
        "        prev_end = prev_chunk.page_content[-30:]\n",
        "        print(f\"Overlap check - Previous chunk ended: '...{prev_end}'\")\n",
        "        print(f\"Overlap check - Current chunk starts: '{current_start}...'\")\n",
        "    print()\n",
        "print(\"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3j7NWMlPUN_",
        "outputId": "bb67a684-5a80-41f3-e567-537f2b717135"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking breakdown:\n",
            "--- CHUNK 1 ---\n",
            "Length: 597 characters\n",
            "Source: doc_1\n",
            "Text (preview): 'Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasizes code readability with its notable use of significant whitespace.     Python supports multiple programming paradigms including procedural, object-o...'\n",
            "\n",
            "--- CHUNK 2 ---\n",
            "Length: 669 characters\n",
            "Source: doc_2\n",
            "Text (preview): 'Machine Learning and Artificial Intelligence:     Machine learning is a subset of artificial intelligence that enables computers to learn from data automatically.     There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.     Supervise...'\n",
            "Overlap check - Previous chunk ended: '...machine learning applications.'\n",
            "Overlap check - Current chunk starts: 'Machine Learning and Artificia...'\n",
            "\n",
            "--- CHUNK 3 ---\n",
            "Length: 629 characters\n",
            "Source: doc_3\n",
            "Text (preview): 'Web Development Technologies:     Web development involves creating websites and web applications using various technologies.     Frontend development focuses on user interfaces using HTML, CSS, and JavaScript.     Backend development handles server-side logic, databases, and APIs using languages li...'\n",
            "  ... and 1 more chunks\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Extract chunk texts for embedding\n",
        "chunks_texts = [chunk.page_content for chunk in chunks]\n",
        "print(f\"Extracted {len(chunks_texts)} chunk texts\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixf7JL-kPcgk",
        "outputId": "c6cfaf5d-05b7-4dd0-cca8-b335bafd158f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 4 chunk texts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Creating Embeddings\n",
        "\n",
        "\n",
        "In this step, we transform text chunks into **vector embeddings** that capture semantic meaning.  \n",
        "These embeddings will later be stored in MongoDB and used for similarity-based retrieval.  \n",
        "\n",
        "The workflow is as follows:  \n",
        "\n",
        "1. **Load Embedding Model**  \n",
        "   - We use **`all-MiniLM-L6-v2`** (from SentenceTransformers), a lightweight and fast model.  \n",
        "   - It produces **384-dimensional embeddings**, inferred automatically from a test run.  \n",
        "\n",
        "2. **Generate Embeddings for Chunks**  \n",
        "   - The `model.encode()` method converts each text chunk into a dense vector.  \n",
        "   - Batching (`batch_size=32`) ensures efficiency without overloading memory.  \n",
        "   - The result is a NumPy array of shape `(num_chunks, embedding_dim)`.  \n",
        "\n",
        "3. **Prepare Documents for MongoDB**  \n",
        "   - Each chunk is packaged into a dictionary with:  \n",
        "     - `_id` → numeric index  \n",
        "     - `text` → chunk content  \n",
        "     - `embedding` → embedding as a Python list  \n",
        "     - `source` → document metadata  \n",
        "   - Sanity checks ensure the embeddings all have the expected length (384).  \n",
        "\n",
        "4. **Preview a Sample Document**  \n",
        "   - The helper function `preview_document()` prints out metadata, text snippet, and embedding size for quick inspection.  \n",
        "\n",
        "At the end of this section, we have a **list of documents (`documents_to_insert`)** ready to be stored in MongoDB with both text and embeddings.  \n"
      ],
      "metadata": {
        "id": "2Z6e664GQqlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a model\n",
        "\n",
        "# all-MiniLM-L6-v2 is a good default (fast, 384-dim)\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "print(\"Loaded SentenceTransformer model:\", MODEL_NAME)\n",
        "\n",
        "# Quick check to infer embedding dimension\n",
        "_sample = model.encode(\"test\", convert_to_numpy=True)\n",
        "EMBED_DIM = int(_sample.shape[0])\n",
        "print(\"Inferred EMBED_DIM (sample):\", EMBED_DIM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "6f3fc2af8a7c443ba50059623fbc41fb",
            "dfcf84d6126d4a87955776a693dcc0c3",
            "48387396fb364ebe90c490974a92a459",
            "42dd855f174b4130963b56b430cf87ab",
            "6fcf07dcbe884e1db42c174f2f949278",
            "bf06e5c1a94b491e8f51464a74fdb687",
            "bfaaa55e097d4c7dbc70a318df9b9a52",
            "613649cbc3f44909991013eeacccbdc0",
            "6c1fbb2cdb164c0abdf049e5989d7dec",
            "76e9f20892ff40e984a7f2064a7fa663",
            "658c879b06924e68a11f29045a878a18",
            "7027e321078e435a88caf1f99cb3cafb",
            "e33617cdb5834536bc5f91e392541c74",
            "8aabd7d4bd9a42c0bae18d355c2c7746",
            "3deb2f31e39c45d3b791ef4f5a160ed5",
            "242ffece477b4a9ab6789cccd157a787",
            "50cb38d7a7a94d3f83324f55c833c171",
            "349549c92fec4ae5a6579ffe45996c49",
            "b83b97ce623b4748888e1d9b16595a57",
            "efe3722446c542aa992e36b52e9c57f8",
            "088a40be17a14366abcb36de95714382",
            "767fa8d323bc4f65842c47ec13157001",
            "16e2ae50afc04bc399cb4bae2c373821",
            "f35c83e5da4d42a49b5e25f3dd0bd4b5",
            "c1ae4e2d67a14990b26ca4dc64d80d1d",
            "9b69e1cd87bf411283e4746975c94f06",
            "879135e2c64c41769e9cd855b49d6e03",
            "9a9c268850014fad823466844d2f2e4c",
            "73cbcb60e650459f8d7c7f534fa1c61a",
            "72a8303e3f83439789298f8da1016254",
            "63acf8d06fe34d8e8804e63f5a11764a",
            "9c7e6a88b47c4f6f9f0631a7672a8733",
            "d2299364781144c7b6c0c1a001a09a20",
            "701b47e6aaaf4ea9a13d719809e5dd74",
            "41e5d1a5380442ff8b69d23394868dd1",
            "ae04227feddb4e10ba02a31d4c5b8297",
            "36e7b760122449ccb562712b7b177b0f",
            "8305fda5419f4d9f917d7a7d05f91d9f",
            "2d0f0973b025447d89b7e38cc9a68a29",
            "aa9f8d4177d946b4b111a072ed7c6a49",
            "6140afc8616746778f9a356b6e5c5ed5",
            "a23c6de9b9334bf39ac94d01b2164d9e",
            "33d81692d72c407d9f55f01431393ea1",
            "fe67c1bef6704a1ea200496ad97bf4bd",
            "ec1b5b4fd940466291610b8f002e7653",
            "5b58a208661f414e9779b124b1b861b4",
            "115e144105064828b1481e7634b27013",
            "dd20abb8380a44dab35fee83c877f506",
            "6ef84c463f0f456fbeb60a2f69ffaf9d",
            "f694f7b23f1444cf9fe79c3361535c0a",
            "c8d4941f82244a90a129c38ae1fe9af4",
            "78e53109fe2c438c8e09ee8279c8e8b6",
            "77af5a11ce1046afb5998840287c2dd3",
            "2b1b027145d34971b11a8a78ad14cba2",
            "f94b6a7c86a74f3cbdbd1fbf6d5d6752",
            "ef4ece17d72345c09103a8d9ec2c06ff",
            "2ba0b3edfb2a4c1a98abc23dcdd54f01",
            "2bd4de3fa9f048f88d62653ec82c6b8a",
            "1f19f6c80b644e36ba915a7256e03437",
            "27129f33d3d141dbb3ab58c5dcd808f3",
            "b09300c8fab44748ab10aa81dc70867f",
            "e9bb848e056f43458b85fe6d40132cca",
            "d5898445cb9245fd98a71df979636799",
            "abe030503d2c400c93d20182495085b9",
            "ecc28ddb1c8e473d9ad50946941e0682",
            "eafb646e6abc4132a2456aad4bb4f0b9",
            "f84a919bd05a4a18bfb42915d8c03279",
            "e4be713d18f84a289ffc52daacf91d37",
            "9711197a929f48a893cfacc045865ef5",
            "5cb2b69139f2418984fbf416fa97d748",
            "1c598966ed49412cafd0f38664149a51",
            "ac583f91da5646a3aa1f57f90ed58cf3",
            "86d9fb8668944aca8672ddfc4845ed72",
            "4b53dbfb86c14ac1a0362d8fa7bfd6e6",
            "027ce0208f44418ea6bd9fe5ff4788ca",
            "2c825b9b35d047b6b76cff1fbd8a78c5",
            "6a10e6e6b2d845eebc0b774745418c01",
            "c54e2d897d0f4c5f83748694efd73cc4",
            "8b2447d9669b4492b1ad641f6ca42116",
            "f2da0fde50c94f6ebd15978735d6d0d6",
            "10d707b23d7e4d2ab27d5b70afcfcd8b",
            "c01846b3e55a4e9c8fb25c12e6e58107",
            "8376cdaca24143b795d45ed6854e2a1d",
            "4a978e3cd5b841d49904b153ed86d5ca",
            "d1ecacb6c75d4c94b2b813af65ae8515",
            "70ded85782d44343a1ab810213ccbc69",
            "1c97dc228eeb4cf1bb481afdcc59f26b",
            "ea98f087059e4c419d169c6de35f24c7",
            "8fdf156487aa45dda93ca526145fcf6c",
            "02492fe7e9f14742bd3419fbb00f8e6b",
            "83eb8d6da1e24563b2bef0cf03537572",
            "aa47d30cb08b495b8f4bddb7a4a1838c",
            "fb1913ebfd2443748cba580be000f19c",
            "1887fb7868244234b0a8d7e427f35046",
            "f545636f60794255be4eef18b8a421eb",
            "071fb98cf15949b7ac5d19664970f57d",
            "77bcef5b0e6e42d381f3f744874bcccc",
            "9a673fa2f98c47ea945fa34bb8341d1e",
            "6d5bdabad4bb47dfb46e6cb51ed0c582",
            "cbf602e8b9da4e2ba5457941d39cc12e",
            "ed64dbcded004a4cbade36a44be98a01",
            "56d354e376bb4ebd8c37e5c3893fcfed",
            "33c58a9750ad4212a5fb03ee0e1a1e5c",
            "63ef9b2ecc4a4f7e9f5639c6186e283c",
            "a8c84058085a4591bff44f0cce8794eb",
            "6116e142ba474ccd87af8d1c7d171547",
            "bdabe225fc354c05bebfe30cb210992b",
            "d20f567ad7754a88999e9e9dec920ee6",
            "746cd9b802c743eaaf663c2f75179126",
            "09e77a8fb91349e5bc3de1631f763df4",
            "753e74823bbf41ceaf293af24ae89ae2",
            "ae4ff0ea73bb4aa58119d16dd63c040a",
            "26a5b33fdb8d45f28d26d42e7430dcef",
            "6da7e41ff02b43f48fe13d54a24019cf",
            "a79ce534220847f78f6267dddc060697",
            "9e961375b7004b53951c87fe1b0c8a0f",
            "00f1f1d2ee45427499f6d33ab51439eb",
            "5b1230da505c464c82f04f4045e6d051",
            "03840ef8a063414b9ea79074d910c7a8",
            "67e85d6c810b495d800121d169582d58",
            "cc050feac3644c7691f4a74353bc2fd2"
          ]
        },
        "id": "iHVn-_2zcrxP",
        "outputId": "fa0411c0-8331-44be-ac5e-f08b7515acea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f3fc2af8a7c443ba50059623fbc41fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7027e321078e435a88caf1f99cb3cafb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16e2ae50afc04bc399cb4bae2c373821"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "701b47e6aaaf4ea9a13d719809e5dd74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec1b5b4fd940466291610b8f002e7653"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef4ece17d72345c09103a8d9ec2c06ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f84a919bd05a4a18bfb42915d8c03279"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c54e2d897d0f4c5f83748694efd73cc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fdf156487aa45dda93ca526145fcf6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbf602e8b9da4e2ba5457941d39cc12e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "753e74823bbf41ceaf293af24ae89ae2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SentenceTransformer model: all-MiniLM-L6-v2\n",
            "Inferred EMBED_DIM (sample): 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Embedding for the chunk_texts (Batched)\n",
        "\n",
        "assert 'chunks_texts' in globals() and isinstance(chunks_texts, list) and len(chunks_texts) > 0, \\\n",
        "    \"chunks_texts missing or empty. Run Section 2 chunking cell first.\"\n",
        "\n",
        "print(f\"Creating embeddings for {len(chunks_texts)} chunks using model '{MODEL_NAME}' ...\")\n",
        "\n",
        "# Choose batch size by memory; 32 is usually safe.\n",
        "batch_size = 32\n",
        "\n",
        "# model.encode accepts a list and handles batching internally\n",
        "doc_embeddings = model.encode(\n",
        "    chunks_texts,\n",
        "    batch_size=batch_size,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "# Convert to numpy & validate shape\n",
        "doc_embeddings = np.asarray(doc_embeddings)\n",
        "if doc_embeddings.ndim != 2:\n",
        "    raise RuntimeError(f\"Unexpected embeddings shape: {doc_embeddings.shape}\")\n",
        "\n",
        "EMBED_DIM = int(doc_embeddings.shape[1])\n",
        "print(f\"Created doc_embeddings with shape: {doc_embeddings.shape}\")\n",
        "print(\"EMBED_DIM (use this for Mongo index numDimensions):\", EMBED_DIM)\n",
        "\n",
        "# Display a small sample for sanity\n",
        "print(\"First embedding sample (first 8 values):\", doc_embeddings[0][:8].tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "2fe274147d914d498af939be64c964bc",
            "698313a7c24e4d30840dbc0cdf38a70a",
            "46ef28716a6c4bfb92611d0fa686ca87",
            "2729c0b8f00f45a89258c54e186e9f84",
            "7dbd661b924c48d68317f942b4bbf035",
            "de3069a0c3b049e3bac4a7d6949de9b7",
            "3defa2ec8dbb4873b1af1c7c973916b5",
            "5b3dcdf553ea49c4832f5cc0c80b8a02",
            "ae27fd03c9824255b4f6f4d0a8701a56",
            "d776548c77014246b6bdf933c8c73bc6",
            "03a59cdfbee541cfafdabb08d1b28bc6"
          ]
        },
        "id": "v6sXFgTfdlP3",
        "outputId": "eb738d7a-5eb6-4f24-d0c3-f99ccc81daf9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating embeddings for 4 chunks using model 'all-MiniLM-L6-v2' ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fe274147d914d498af939be64c964bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created doc_embeddings with shape: (4, 384)\n",
            "EMBED_DIM (use this for Mongo index numDimensions): 384\n",
            "First embedding sample (first 8 values): [-0.04258281737565994, -0.013280064798891544, -0.022173451259732246, 0.010173977352678776, -0.018621109426021576, -0.10375276952981949, -0.02200467884540558, 0.029502354562282562]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare documents for Mongo insertion (convert embeddings to lists)\n",
        "\n",
        "\n",
        "assert 'chunks' in globals() and len(chunks) > 0, \"Run chunking first (Section 2).\"\n",
        "assert 'doc_embeddings' in globals() and getattr(doc_embeddings, \"shape\", None), \"Run embedding cell first.\"\n",
        "\n",
        "documents_to_insert = []\n",
        "for i, (chunk, emb) in enumerate(zip(chunks, doc_embeddings)):\n",
        "    documents_to_insert.append({\n",
        "        \"_id\": i,\n",
        "        \"text\": chunk.page_content,\n",
        "        \"embedding\": emb.tolist(),   # convert numpy array -> plain Python list\n",
        "        \"source\": chunk.metadata.get(\"source\", f\"doc_{i+1}\")\n",
        "    })\n",
        "\n",
        "print(f\"Prepared {len(documents_to_insert)} documents for insertion into MongoDB.\")\n",
        "\n",
        "# Sanity checks: counts & embedding lengths\n",
        "assert len(documents_to_insert) == doc_embeddings.shape[0], \"Mismatch: docs vs embeddings\"\n",
        "bad = [ (i, len(d['embedding'])) for i,d in enumerate(documents_to_insert) if len(d['embedding']) != EMBED_DIM ]\n",
        "if bad:\n",
        "    print(\"Found docs with wrong embedding length (first 5):\", bad[:5])\n",
        "else:\n",
        "    print(\"All prepared document embeddings have correct length:\", EMBED_DIM)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYn833pDeMsK",
        "outputId": "65088a22-a0d3-4dcf-c0d2-5631d361a416"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 4 documents for insertion into MongoDB.\n",
            "All prepared document embeddings have correct length: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preview_document(doc, limit=500):\n",
        "    \"\"\"\n",
        "    Pretty-print a single document's metadata, text, and embedding info.\n",
        "\n",
        "    Args:\n",
        "        doc (dict): A document with keys \"_id\", \"source\", \"text\", \"embedding\".\n",
        "        limit (int): Number of characters of text to preview (default: 500).\n",
        "    \"\"\"\n",
        "    if not doc:\n",
        "        print(\"⚠️ Empty or None document passed to preview_document\")\n",
        "        return\n",
        "\n",
        "    text = doc.get(\"text\", \"\")\n",
        "    emb = doc.get(\"embedding\", [])\n",
        "\n",
        "    print(\"\\n--- DOCUMENT PREVIEW ---\")\n",
        "    print(f\"ID: {doc.get('_id')} | Source: {doc.get('source')}\")\n",
        "    print(f\"Text length: {len(text)} chars | Embedding length: {len(emb)}\")\n",
        "\n",
        "    # Show repr to reveal whitespace/newlines\n",
        "    print(\"\\nrepr(text):\")\n",
        "    print(repr(text[:limit]))\n",
        "\n",
        "    # Readable preview: replace newlines with ␤\n",
        "    print(\"\\nReadable preview (first {0} chars, newlines as ␤):\".format(limit))\n",
        "    print(text[:limit].replace(\"\\n\", \"␤\"))\n",
        "\n",
        "documents_to_insert[0]\n",
        "preview_document(documents_to_insert[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl_JmHjJj44k",
        "outputId": "f6208666-eef2-407a-a969-0a2d787fe214"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DOCUMENT PREVIEW ---\n",
            "ID: 0 | Source: doc_1\n",
            "Text length: 597 chars | Embedding length: 384\n",
            "\n",
            "repr(text):\n",
            "'Python Programming Language Overview:\\n    Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.\\n    It emphasizes code readability with its notable use of significant whitespace.\\n    Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\\n    The language is widely used for web development, data analysis, artificial intelligence, and automation.\\n    Popular Python frameworks include Django for web d'\n",
            "\n",
            "Readable preview (first 500 chars, newlines as ␤):\n",
            "Python Programming Language Overview:␤    Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.␤    It emphasizes code readability with its notable use of significant whitespace.␤    Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.␤    The language is widely used for web development, data analysis, artificial intelligence, and automation.␤    Popular Python frameworks include Django for web d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Create the MonogoDB Vector database\n",
        "\n",
        "\n",
        "In this section, we set up **MongoDB Atlas** as the vector database to store and retrieve embeddings.  \n",
        "\n",
        "The workflow covers five steps:  \n",
        "1. **Imports & Config** → Load `pymongo`, set the vector index name (`INDEX_NAME`).  \n",
        "2. **Validate Embedding Dimension** → Ensure `EMBED_DIM` from the embedding model is available for index creation.  \n",
        "3. **Connect to MongoDB Atlas** → Read the connection string, connect to Atlas, and select the target database (`rag_db`) and collection (`chunks`).  \n",
        "4. **Insert Documents & Define Index** → Insert all chunk embeddings into MongoDB, then output a JSON schema for creating an **Atlas Vector Search index** (cosine similarity, dimension = `EMBED_DIM`).  \n",
        "5. **Search Helpers & Test** → Define helper functions for querying: Atlas vector search (preferred) and manual cosine similarity (fallback). Finally, run a sample query to validate retrieval.  \n",
        "\n",
        "By the end of this section, the knowledge base is stored in MongoDB with vector search enabled, making it ready for retrieval in the RAG pipeline.  \n",
        "\n"
      ],
      "metadata": {
        "id": "PyIZofa8R2jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and config\n",
        "\n",
        "from pymongo import MongoClient\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Names / settings\n",
        "INDEX_NAME = \"vector_index\"   # Atlas index name you will create\n",
        "print(\"INDEX_NAME:\", INDEX_NAME)\n",
        "\n",
        "# EMBED_DIM\n",
        "assert 'EMBED_DIM' in globals(), \"Run embeddings section first to set EMBED_DIM.\"\n",
        "print(\"EMBED_DIM (from embeddings):\", EMBED_DIM)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLqg2iMwlRmO",
        "outputId": "bb5e5d10-0190-446c-f6ca-447798f119ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INDEX_NAME: vector_index\n",
            "EMBED_DIM (from embeddings): 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to MongoDB Atlas\n",
        "\n",
        "MONGODB_URL = os.getenv(\"MONGODB_URL\") or None\n",
        "if MONGODB_URL is None:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        MONGODB_URL = userdata.get('MONGODB_URL')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "assert MONGODB_URL and MONGODB_URL.startswith(\"mongodb\"), \"MONGODB_URL missing or invalid. Set it in env or Colab secrets.\"\n",
        "\n",
        "mongo_client = MongoClient(MONGODB_URL)\n",
        "mongo_client.admin.command('ping')   # will raise if connection fails\n",
        "print(\"Connected to MongoDB Atlas\")\n",
        "\n",
        "db = mongo_client[\"rag_db\"]\n",
        "collection = db[\"chunks\"]\n",
        "print(\"Using database:\", db.name, \"collection:\", collection.name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtunIuqOlYnY",
        "outputId": "1b41544a-fa72-43e0-e126-ffa2e0230795"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to MongoDB Atlas\n",
            "Using database: rag_db collection: chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert chunks and embeddings into Mongo\n",
        "\n",
        "assert 'chunks' in globals() and len(chunks) > 0\n",
        "assert 'doc_embeddings' in globals() and getattr(doc_embeddings, \"shape\", None)\n",
        "\n",
        "# Build Mongo documents\n",
        "documents_to_insert = [\n",
        "    {\n",
        "        \"_id\": i,\n",
        "        \"text\": chunk.page_content,\n",
        "        \"embedding\": emb.tolist(),\n",
        "        \"source\": chunk.metadata.get(\"source\", f\"doc_{i+1}\")\n",
        "    }\n",
        "    for i, (chunk, emb) in enumerate(zip(chunks, doc_embeddings))\n",
        "]\n",
        "\n",
        "print(\"Prepared documents:\", len(documents_to_insert))\n",
        "\n",
        "# Clear collection and insert\n",
        "del_res = collection.delete_many({})\n",
        "print(\"Cleared existing documents:\", del_res.deleted_count)\n",
        "\n",
        "ins = collection.insert_many(documents_to_insert)\n",
        "print(\"Inserted documents:\", len(ins.inserted_ids))\n",
        "\n",
        "# Quick sanity sample\n",
        "sample = collection.find_one({}, {\"_id\":1,\"source\":1,\"text\":1,\"embedding\":1})\n",
        "print(\"Sample in DB →\", {\n",
        "    \"_id\": sample[\"_id\"],\n",
        "    \"source\": sample.get(\"source\"),\n",
        "    \"text_len\": len(sample[\"text\"]),\n",
        "    \"embedding_len\": len(sample[\"embedding\"])\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO4jj_Ejlt6Q",
        "outputId": "a46ef4eb-72c0-4ffb-c9ed-6102d8a88946"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared documents: 4\n",
            "Cleared existing documents: 4\n",
            "Inserted documents: 4\n",
            "Sample in DB → {'_id': 0, 'source': 'doc_1', 'text_len': 597, 'embedding_len': 384}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Atlas vector index definition\n",
        "\n",
        "vector_index_definition = {\n",
        "  \"fields\": [\n",
        "    {\"type\": \"vector\", \"path\": \"embedding\", \"numDimensions\": EMBED_DIM, \"similarity\": \"cosine\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "print(\"Recommended Atlas Vector Search definition (use in Atlas UI):\")\n",
        "print(json.dumps(vector_index_definition, indent=2))\n",
        "\n",
        "#------ Create this index in Atlas manually:\n",
        "#   Cluster → Search → Create Search Index → Atlas Vector Search\n",
        "#   Database: rag_db\n",
        "#   Collection: chunks\n",
        "#   Index Name: vector_index\n",
        "#   Paste the JSON above, then wait until status = ACTIVE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyJjSD0al-qA",
        "outputId": "bb7220e1-4d65-403a-ca46-75962497441e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended Atlas Vector Search definition (use in Atlas UI):\n",
            "{\n",
            "  \"fields\": [\n",
            "    {\n",
            "      \"type\": \"vector\",\n",
            "      \"path\": \"embedding\",\n",
            "      \"numDimensions\": 384,\n",
            "      \"similarity\": \"cosine\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search helpers (Atlas + fallback)\n",
        "\n",
        "def mongodb_vector_search(query_text, top_k=3, debug=False):\n",
        "    \"\"\"Try Atlas Vector Search first. Returns [] if fails.\"\"\"\n",
        "    try:\n",
        "        q_emb = model.encode([query_text], convert_to_numpy=True)[0].tolist()\n",
        "        pipeline = [\n",
        "            {\"$vectorSearch\": {\n",
        "                \"index\": INDEX_NAME,\n",
        "                \"path\": \"embedding\",\n",
        "                \"queryVector\": q_emb,\n",
        "                \"numCandidates\": top_k * 5,\n",
        "                \"limit\": top_k\n",
        "            }},\n",
        "            {\"$project\": {\"_id\":1,\"text\":1,\"source\":1,\"score\":{\"$meta\":\"vectorSearchScore\"}}}\n",
        "        ]\n",
        "        return list(collection.aggregate(pipeline))\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(\"Atlas vector search failed:\", str(e))\n",
        "        return []\n",
        "\n",
        "\n",
        "def fallback_search(query_text, top_k=3):\n",
        "    \"\"\"Manual cosine similarity over stored embeddings.\"\"\"\n",
        "    qemb = np.asarray(model.encode([query_text], convert_to_numpy=True)[0], dtype=np.float32)\n",
        "    qnorm = np.linalg.norm(qemb)\n",
        "    sims = []\n",
        "    for d in collection.find({}, {\"_id\":1,\"text\":1,\"source\":1,\"embedding\":1}):\n",
        "        d_emb = np.asarray(d[\"embedding\"], dtype=np.float32)\n",
        "        denom = qnorm * np.linalg.norm(d_emb)\n",
        "        score = float(np.dot(qemb, d_emb) / denom) if denom != 0 else 0.0\n",
        "        sims.append({\"_id\": d[\"_id\"], \"text\": d[\"text\"], \"source\": d[\"source\"], \"score\": score})\n",
        "    return sorted(sims, key=lambda x: x[\"score\"], reverse=True)[:top_k]\n",
        "\n",
        "\n",
        "def test_search(query=\"What is Python programming?\", top_k=3):\n",
        "    \"\"\"Run a query, show Atlas results if available, else fallback.\"\"\"\n",
        "    print(f\"\\n🔎 Query: {query}\")\n",
        "    results = mongodb_vector_search(query, top_k=top_k)\n",
        "    if results:\n",
        "        print(\"[ATLAS] Results:\")\n",
        "        for i, r in enumerate(results, 1):\n",
        "            print(f\"{i}. score={r.get('score'):.4f} | source={r.get('source')}\")\n",
        "            print(\"   preview:\", r.get('text','')[:200].replace(\"\\n\",\" \"))\n",
        "    else:\n",
        "        print(\"⚠ [FALLBACK] Atlas not available, using cosine similarity:\")\n",
        "        results = fallback_search(query, top_k=top_k)\n",
        "        for i, r in enumerate(results, 1):\n",
        "            print(f\"{i}. score={r['score']:.4f} | source={r['source']}\")\n",
        "            print(\"   preview:\", r['text'][:200].replace(\"\\n\",\" \"))\n",
        "    return results\n",
        "\n",
        "\n",
        "# Run a sample query\n",
        "test_search(\"What is Python programming?\", top_k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzbi3AZsutWL",
        "outputId": "cb55dfb9-0743-44e4-c792-c02dca24d242"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Query: What is Python programming?\n",
            "[ATLAS] Results:\n",
            "1. score=0.9127 | source=doc_1\n",
            "   preview: Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasizes code readability with its notable use of signi\n",
            "2. score=0.7424 | source=doc_4\n",
            "   preview: Discord Bot Development:     Discord bots are applications that can interact with Discord servers automatically.     They can respond to messages, moderate chat, play music, and perform various automa\n",
            "3. score=0.6612 | source=doc_3\n",
            "   preview: Web Development Technologies:     Web development involves creating websites and web applications using various technologies.     Frontend development focuses on user interfaces using HTML, CSS, and J\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'_id': 0,\n",
              "  'text': 'Python Programming Language Overview:\\n    Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.\\n    It emphasizes code readability with its notable use of significant whitespace.\\n    Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\\n    The language is widely used for web development, data analysis, artificial intelligence, and automation.\\n    Popular Python frameworks include Django for web development, NumPy for scientific computing,\\n    and TensorFlow for machine learning applications.',\n",
              "  'source': 'doc_1',\n",
              "  'score': 0.9126845002174377},\n",
              " {'_id': 3,\n",
              "  'text': \"Discord Bot Development:\\n    Discord bots are applications that can interact with Discord servers automatically.\\n    They can respond to messages, moderate chat, play music, and perform various automated tasks.\\n    Discord bots are built using Discord's API and can be developed in multiple programming languages.\\n    Python developers often use the discord.py library to create bots with features like slash commands.\\n    Bots require proper authentication using bot tokens and must be invited to servers with appropriate permissions.\\n    Common bot features include welcome messages, role management, music playback, and custom commands.\",\n",
              "  'source': 'doc_4',\n",
              "  'score': 0.742400586605072},\n",
              " {'_id': 2,\n",
              "  'text': 'Web Development Technologies:\\n    Web development involves creating websites and web applications using various technologies.\\n    Frontend development focuses on user interfaces using HTML, CSS, and JavaScript.\\n    Backend development handles server-side logic, databases, and APIs using languages like Python, Java, or Node.js.\\n    REST APIs provide a way for different systems to communicate over HTTP using standard methods.\\n    Modern web frameworks like React, Vue.js, and Angular help build interactive user interfaces.\\n    Database systems like PostgreSQL, MongoDB, and Redis store and manage application data efficiently.',\n",
              "  'source': 'doc_3',\n",
              "  'score': 0.6612247228622437}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: LLM integration and prompt engineering\n",
        "\n",
        "\n",
        "In this section, we connect the pipeline to **Azure OpenAI GPT-3.5 Turbo** and design structured prompts to guide the model’s responses.  \n",
        "The goal is to make the LLM answer strictly from retrieved context while avoiding hallucination.  \n",
        "\n",
        "The workflow covers five steps:  \n",
        "\n",
        "1. **Azure OpenAI Setup**  \n",
        "   - Import the `AzureOpenAI` client.  \n",
        "   - Load API credentials (`AZURE_OPENAI_KEY`, endpoint, deployment name, API version) from Colab secrets.  \n",
        "   - Initialize the client to enable chat completions.  \n",
        "\n",
        "2. **Prompt Templates**  \n",
        "   - Define a **system prompt** (instructs the model to only use provided context and cite sources).  \n",
        "   - Define a **user prompt template** with placeholders for `{context}` and `{question}`.  \n",
        "\n",
        "3. **Context Builder**  \n",
        "   - Implement `build_context_from_docs()` to assemble retrieved documents into a concise context string.  \n",
        "   - Each snippet is prefixed with `[source:doc_X]` to support inline citations.  \n",
        "\n",
        "4. **Azure Chat Wrapper**  \n",
        "   - Create `call_azure_chat()`, a helper function that sends system and user prompts to Azure OpenAI.  \n",
        "   - Handles parameters like `max_tokens`, `temperature`, and error cases with optional debug printing.  \n",
        "\n",
        "5. **RAG Query Helper**  \n",
        "   - Define `rag_query()` to tie everything together:  \n",
        "     - Retrieve top-k documents (Atlas or fallback).  \n",
        "     - Build context from docs.  \n",
        "     - Format the user prompt.  \n",
        "     - Call the LLM and extract inline sources.  \n",
        "   - Returns a structured result with the answer, sources, and retrieved documents.  \n",
        "\n",
        "By the end of this section, the chatbot can **retrieve context + generate grounded answers** using Azure OpenAI, with proper source citations.  \n"
      ],
      "metadata": {
        "id": "kwaU1GkIzxxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & Azure OpenAI config\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "import os, traceback\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load Azure key\n",
        "\n",
        "print(\"Azure OpenAI setup starting...\")\n",
        "\n",
        "os.environ['AZURE_OPENAI_KEY '] = userdata.get('AZURE_OPENAI_KEY')\n",
        "AZURE_OPENAI_ENDPOINT = \"https://jibz3-mfxwvj2c-swedencentral.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview\"  # From your screenshot\n",
        "AZURE_OPENAI_KEY = userdata.get('AZURE_OPENAI_KEY')\n",
        "AZURE_DEPLOYMENT_NAME = \"gpt-35-turbo\"\n",
        "AZURE_API_VERSION = \"2024-12-01-preview\"\n",
        "\n",
        "assert AZURE_OPENAI_KEY, \"AZURE_OPENAI_KEY not found in env/Colab secrets.\"\n",
        "assert AZURE_OPENAI_ENDPOINT, \"AZURE_OPENAI_ENDPOINT not found. Set your Azure OpenAI endpoint URL.\"\n",
        "\n",
        "print(\"Azure OpenAI configuration loaded. Deployment:\", AZURE_DEPLOYMENT_NAME)\n",
        "\n",
        "# Initialize client\n",
        "client = AzureOpenAI(\n",
        "    api_key=AZURE_OPENAI_KEY,\n",
        "    base_url=AZURE_OPENAI_ENDPOINT,\n",
        "    api_version=AZURE_API_VERSION\n",
        ")\n",
        "print(\"AzureOpenAI client initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf8Ni2nbz41v",
        "outputId": "26911235-e400-4a10-d930-b82dcb0141dd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure OpenAI setup starting...\n",
            "Azure OpenAI configuration loaded. Deployment: gpt-35-turbo\n",
            "AzureOpenAI client initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt templates & context formatter\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an assistant that answers user questions using ONLY the provided CONTEXT. \"\n",
        "    \"Cite sources inline using [source:doc_X]. If the answer cannot be found in the context, \"\n",
        "    \"say 'I don't know' and do not hallucinate.\"\n",
        ")\n",
        "\n",
        "# This template will receive {context} and {question}\n",
        "\n",
        "USER_PROMPT_TEMPLATE = (\n",
        "    \"CONTEXT:\\n{context}\\n\\n\"\n",
        "    \"QUESTION:\\n{question}\\n\\n\"\n",
        "    \"INSTRUCTIONS:\\n\"\n",
        "    \"- Answer based only on the CONTEXT above.\\n\"\n",
        "    \"- Keep the answer concise and include source tags like [source:doc_1].\\n\"\n",
        "    \"- If context doesn't contain the answer, reply: 'I don't know'.\\n\\n\"\n",
        "    \"ANSWER:\"\n",
        ")\n",
        "\n",
        "def build_context_from_docs(docs, per_doc_chars=500):\n",
        "    \"\"\"\n",
        "    Build a single context string from retrieved docs.\n",
        "    Each doc is a dict with keys: _id, text, source, score (if present).\n",
        "    Truncate each doc to per_doc_chars characters to keep prompts small.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for d in docs:\n",
        "        src = d.get(\"source\") or f\"doc_{d.get('_id')}\"\n",
        "        text = d.get(\"text\", \"\")\n",
        "        # clean whitespace and truncate\n",
        "        text_snippet = text.strip().replace(\"\\n\", \" \")[:per_doc_chars].strip()\n",
        "        parts.append(f\"[source:{src}] {text_snippet}\")\n",
        "    return \"\\n\\n\".join(parts)\n"
      ],
      "metadata": {
        "id": "c6Gfa5qHNlDb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Azure call wrapper\n",
        "\n",
        "def call_azure_chat(prompt_system, prompt_user, max_tokens=350, temperature=0.0, debug=False):\n",
        "    \"\"\"\n",
        "    Calls Azure OpenAI chat completion. Returns the assistant text.\n",
        "    prompt_system: system message string\n",
        "    prompt_user: user message string (full prompt including context + question)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": prompt_system},\n",
        "            {\"role\": \"user\", \"content\": prompt_user}\n",
        "        ]\n",
        "        if debug:\n",
        "            print(\"Calling Azure with messages (truncated):\")\n",
        "            print(\"SYSTEM:\", prompt_system[:300])\n",
        "            print(\"USER:\", prompt_user[:800])\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=AZURE_DEPLOYMENT_NAME,\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        # Extract text safely\n",
        "        text = response.choices[0].message.content\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            traceback.print_exc()\n",
        "        return f\"[LLM_ERROR] {str(e)}\"\n"
      ],
      "metadata": {
        "id": "wL5tjnCUNzRk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  RAG helper: retrieval + LLM generation\n",
        "\n",
        "def rag_query(user_question, top_k=3, per_doc_chars=500, use_atlas=True, llm_debug=False):\n",
        "    \"\"\"\n",
        "    End-to-end RAG:\n",
        "      1) retrieve top_k docs (Atlas vector search )\n",
        "      2) build context string\n",
        "      3) call LLM with system + user prompt\n",
        "    Returns: dict with keys: answer, sources (list), docs (retrieved)\n",
        "    \"\"\"\n",
        "    # 1) Retreive: try Atlas first, fallback to manual cosine if needed\n",
        "    docs = []\n",
        "    if use_atlas:\n",
        "        try:\n",
        "            docs = mongodb_vector_search(user_question, top_k=top_k)\n",
        "        except Exception:\n",
        "            docs = []\n",
        "    if not docs:\n",
        "        docs = fallback_search(user_question, top_k=top_k)\n",
        "\n",
        "    # normalize docs: ensure dicts have _id,text,source\n",
        "    normalized = []\n",
        "    for d in docs:\n",
        "        # If Atlas returns Mongo docs (with score meta), keep keys consistent\n",
        "        normalized.append({\n",
        "            \"_id\": d.get(\"_id\"),\n",
        "            \"text\": d.get(\"text\") or d.get(\"page_content\") or \"\",\n",
        "            \"source\": d.get(\"source\") or f\"doc_{d.get('_id')}\",\n",
        "            \"score\": d.get(\"score\")\n",
        "        })\n",
        "\n",
        "    # 2) Build context\n",
        "    context = build_context_from_docs(normalized, per_doc_chars=per_doc_chars)\n",
        "\n",
        "    # 3) Build full prompt and call LLM\n",
        "    user_prompt = USER_PROMPT_TEMPLATE.format(context=context, question=user_question)\n",
        "    answer = call_azure_chat(SYSTEM_PROMPT, user_prompt, debug=llm_debug)\n",
        "\n",
        "    # Extract used sources from answer heuristically (simple)\n",
        "    used_sources = []\n",
        "    for part in normalized:\n",
        "        tag = f\"[source:{part['source']}]\"\n",
        "        if tag in answer:\n",
        "            used_sources.append(part['source'])\n",
        "\n",
        "    # If model responded with hallucination marker or error, you might enforce fallback text\n",
        "    if answer.strip().lower().startswith(\"[llm_error]\"):\n",
        "        final_answer = \"Error from LLM: \" + answer\n",
        "    else:\n",
        "        final_answer = answer\n",
        "\n",
        "    return {\"answer\": final_answer, \"sources\": used_sources, \"docs\": normalized, \"context\": context}\n"
      ],
      "metadata": {
        "id": "ITwgwcRvOcnD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example uses\n",
        "\n",
        "q = \"Who created Python and when?\"\n",
        "res = rag_query(q, top_k=3, per_doc_chars=400, llm_debug=False)\n",
        "print(\"\\n=== RAG ANSWER ===\")\n",
        "print(res[\"answer\"])\n",
        "print(\"\\nSources used (detected):\", res[\"sources\"])\n",
        "print(\"\\nRetrieved docs (ids & sources):\")\n",
        "for d in res[\"docs\"]:\n",
        "    print(d[\"_id\"], d[\"source\"], f\"(score={d.get('score')})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9d719a8OuEl",
        "outputId": "59001d0b-2b32-4723-d904-33231b09af5e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== RAG ANSWER ===\n",
            "Python was created by Guido van Rossum in 1991 [source:doc_1].\n",
            "\n",
            "Sources used (detected): ['doc_1']\n",
            "\n",
            "Retrieved docs (ids & sources):\n",
            "0 doc_1 (score=0.8238394856452942)\n",
            "3 doc_4 (score=0.6892796754837036)\n",
            "2 doc_3 (score=0.5887914896011353)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: RAG pipeline\n",
        "\n",
        "\n",
        "In this section, we bring together all previous components (embeddings, MongoDB, and LLM) into a full **Retrieval-Augmented Generation (RAG) pipeline**.  \n",
        "The pipeline handles retrieval, context-building, prompt creation, and LLM response in a single flow.  \n",
        "\n",
        "The workflow covers four steps:  \n",
        "\n",
        "1. **Retrieve Documents for a Query**  \n",
        "   - `retrieve_docs_for_query()` encodes the user query with the SentenceTransformer model.  \n",
        "   - Attempts **Atlas Vector Search** first; if it fails, falls back to manual cosine similarity.  \n",
        "   - Returns a normalized list of docs with `_id`, `text`, `source`, and `score`.  \n",
        "\n",
        "2. **Build RAG Prompt**  \n",
        "   - `build_rag_prompt()` assembles the retrieved docs into a context string.  \n",
        "   - Uses the system + user templates from Section 5.  \n",
        "   - Returns a dict with: system message, user prompt, and context.  \n",
        "\n",
        "3. **Call LLM**  \n",
        "   - `call_llm_for_rag()` is a thin wrapper around the Azure helper.  \n",
        "   - Keeps the pipeline modular by separating LLM calling logic from retrieval.  \n",
        "\n",
        "4. **Run Full RAG Pipeline**  \n",
        "   - `run_rag_pipeline()` orchestrates the entire workflow:  \n",
        "     - Retrieve docs → Build prompt → Query LLM → Extract cited sources.  \n",
        "   - Handles edge cases (e.g., if the LLM errors, a safe error message is returned).  \n",
        "   - Returns a structured result with `question`, `answer`, `docs`, `sources`, and `context`.  \n",
        "   - A quick test query (“What is Python programming?”) validates the flow end-to-end.  \n",
        "\n",
        "By the end of this section, the system can answer user questions using **retrieved context + LLM reasoning** in one unified function.  \n"
      ],
      "metadata": {
        "id": "q2zDPAkfVC06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Embed Query & Retrieve (Atlas first, fallback)\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "def retrieve_docs_for_query(query: str, top_k: int = 3, debug: bool = False) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    1) Encode the query using the SentenceTransformer `model`.\n",
        "    2) Attempt Atlas vector search. If it returns nothing or fails, use fallback cosine search.\n",
        "    3) Return a list of retrieved doc dicts with keys: _id, text, source, score (score may be None).\n",
        "    \"\"\"\n",
        "    # 1) embed query\n",
        "    q_emb = model.encode([query], convert_to_numpy=True)[0].tolist()\n",
        "    if debug:\n",
        "        print(f\"[retrieve] query embedded (len={len(q_emb)})\")\n",
        "\n",
        "    # 2) Try Atlas\n",
        "    docs = []\n",
        "    try:\n",
        "        docs = mongodb_vector_search(query, top_k=top_k)\n",
        "        if debug:\n",
        "            print(f\"[retrieve] Atlas returned {len(docs)} docs\")\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(\"[retrieve] Atlas search exception:\", e)\n",
        "        docs = []\n",
        "\n",
        "    # 3) Fallback if Atlas returned nothing\n",
        "    if not docs:\n",
        "        if debug:\n",
        "            print(\"[retrieve] Using fallback_search (manual cosine)\")\n",
        "        docs = fallback_search(query, top_k=top_k)\n",
        "\n",
        "    # Normalize returned docs to expected dict shape\n",
        "    normalized = []\n",
        "    for d in docs:\n",
        "        normalized.append({\n",
        "            \"_id\": d.get(\"_id\"),\n",
        "            \"text\": d.get(\"text\") or d.get(\"page_content\") or \"\",\n",
        "            \"source\": d.get(\"source\") or f\"doc_{d.get('_id')}\",\n",
        "            \"score\": d.get(\"score\")\n",
        "        })\n",
        "    return normalized\n"
      ],
      "metadata": {
        "id": "djppossDVJL-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prompt Builder (assemble retrieved chunks into a context)\n",
        "\n",
        "def build_rag_prompt(question: str,\n",
        "                     docs: List[Dict],\n",
        "                     per_doc_chars: int = 500,\n",
        "                     system_prompt: str = None,\n",
        "                     user_template: str = None) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Build the system + user messages for the LLM from the question and retrieved docs.\n",
        "    Returns a dict: {'system': system_prompt, 'user': user_prompt, 'context': context_str}\n",
        "    \"\"\"\n",
        "    # Use templates from Section 5 if not provided\n",
        "    sys_prompt = system_prompt or SYSTEM_PROMPT\n",
        "    user_tmpl = user_template or USER_PROMPT_TEMPLATE\n",
        "\n",
        "    # Build context (reuse helper from Section 5)\n",
        "    context_str = build_context_from_docs(docs, per_doc_chars=per_doc_chars)\n",
        "\n",
        "    # Fill the user prompt template\n",
        "    user_prompt = user_tmpl.format(context=context_str, question=question)\n",
        "\n",
        "    return {\"system\": sys_prompt, \"user\": user_prompt, \"context\": context_str}\n"
      ],
      "metadata": {
        "id": "8CK665avVUsB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# LLM Call\n",
        "\n",
        "def call_llm_for_rag(system_msg: str, user_msg: str, max_tokens: int = 350, temperature: float = 0.0, debug: bool = False):\n",
        "    \"\"\"\n",
        "    Thin wrapper around the Azure call helper. Keeps the interface clear for the pipeline.\n",
        "    Returns the LLM string answer (raw).\n",
        "    \"\"\"\n",
        "    # We reuse call_azure_chat from Section 5 which already handles errors and debug printing\n",
        "    return call_azure_chat(system_msg, user_msg, max_tokens=max_tokens, temperature=temperature, debug=debug)\n"
      ],
      "metadata": {
        "id": "6xTdeJ2kVdT2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Full RAG pipeline wrapper (retrieve → prompt → LLM → return)\n",
        "\n",
        "def run_rag_pipeline(question: str,\n",
        "                     top_k: int = 3,\n",
        "                     per_doc_chars: int = 500,\n",
        "                     max_tokens: int = 350,\n",
        "                     temperature: float = 0.0,\n",
        "                     use_atlas: bool = True,\n",
        "                     debug: bool = False) -> Dict:\n",
        "    \"\"\"\n",
        "    End-to-end RAG pipeline.\n",
        "    Returns a dictionary:\n",
        "      {\n",
        "        \"question\": str,\n",
        "        \"answer\": str,\n",
        "        \"docs\": [retrieved docs],\n",
        "        \"sources\": [list of detected sources],\n",
        "        \"context\": str\n",
        "      }\n",
        "    \"\"\"\n",
        "    # 1) Retrieve\n",
        "    docs = retrieve_docs_for_query(question, top_k=top_k, debug=debug if debug else False)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[RAG] Retrieved {len(docs)} docs. ids:\", [d['_id'] for d in docs])\n",
        "\n",
        "    # 2) Build prompt\n",
        "    prompt_obj = build_rag_prompt(question, docs, per_doc_chars=per_doc_chars)\n",
        "    if debug:\n",
        "        print(\"[RAG] Context preview:\", prompt_obj['context'][:400])\n",
        "\n",
        "    # 3) Call LLM\n",
        "    answer = call_llm_for_rag(prompt_obj['system'], prompt_obj['user'], max_tokens=max_tokens, temperature=temperature, debug=debug)\n",
        "\n",
        "    # 4) Extract sources heuristically from answer (detect [source:doc_X] tags)\n",
        "    used_sources = []\n",
        "    for d in docs:\n",
        "        tag = f\"[source:{d['source']}]\"\n",
        "        if tag in answer:\n",
        "            used_sources.append(d['source'])\n",
        "\n",
        "    # 5) Safety: if LLM responded with an error marker, convert to safe message\n",
        "    if isinstance(answer, str) and answer.lower().startswith(\"[llm_error]\"):\n",
        "        final_answer = \"Error from LLM: \" + answer\n",
        "    else:\n",
        "        final_answer = answer\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer\": final_answer,\n",
        "        \"docs\": docs,\n",
        "        \"sources\": used_sources,\n",
        "        \"context\": prompt_obj[\"context\"]\n",
        "    }\n",
        "\n",
        "\n",
        "# Quick test (example)\n",
        "\n",
        "if __name__ == \"__main__\" or True:\n",
        "    demo_q = \"What is Python programming?\"\n",
        "    res = run_rag_pipeline(demo_q, top_k=3, per_doc_chars=400, debug=True)\n",
        "    print(\"\\n=== RAG RESULT ===\")\n",
        "    print(\"Answer:\\n\", res[\"answer\"])\n",
        "    print(\"\\nSources:\", res[\"sources\"])\n",
        "    print(\"\\nRetrieved docs ids:\", [d[\"_id\"] for d in res[\"docs\"]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3HidUPzVmuT",
        "outputId": "b58b2a40-4806-4c99-ed12-999fbc5d29a4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[retrieve] query embedded (len=384)\n",
            "[retrieve] Atlas returned 3 docs\n",
            "[RAG] Retrieved 3 docs. ids: [0, 3, 2]\n",
            "[RAG] Context preview: [source:doc_1] Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasizes code readability with its notable use of significant whitespace.     Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.     The language is widely used for web developm\n",
            "Calling Azure with messages (truncated):\n",
            "SYSTEM: You are an assistant that answers user questions using ONLY the provided CONTEXT. Cite sources inline using [source:doc_X]. If the answer cannot be found in the context, say 'I don't know' and do not hallucinate.\n",
            "USER: CONTEXT:\n",
            "[source:doc_1] Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasizes code readability with its notable use of significant whitespace.     Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.     The language is widely used for web development, data analy\n",
            "\n",
            "[source:doc_4] Discord Bot Development:     Discord bots are applications that can interact with Discord servers automatically.     They can respond to messages, moderate chat, play music, and perform various automated tasks.     Discord bots are built using Discord's API and can be developed in multiple programming languages.     Python developers often use the discord.p\n",
            "\n",
            "=== RAG RESULT ===\n",
            "Answer:\n",
            " Python is a high-level, interpreted programming language created by Guido van Rossum in 1991. It supports multiple programming paradigms and is widely used for web development [source:doc_1].\n",
            "\n",
            "Sources: ['doc_1']\n",
            "\n",
            "Retrieved docs ids: [0, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: RAG Chatbot\n",
        "\n",
        "\n",
        "In this section, we extend the RAG pipeline into a **multi-turn conversational chatbot**.  \n",
        "Unlike the single-question pipeline in Section 6, this chatbot maintains **conversation memory**, allowing it to respond contextually across turns.  \n",
        "\n",
        "The workflow includes four parts:  \n",
        "\n",
        "1. **Conversation Memory Setup**  \n",
        "   - Initialize `conversation_history` as a list of user/assistant messages.  \n",
        "   - This memory lets the assistant recall prior context in an ongoing conversation.  \n",
        "\n",
        "2. **Chatbot Turn Function**  \n",
        "   - `_format_history_for_prompt()` compresses recent history into a short snippet for the LLM.  \n",
        "   - `chatbot_turn()` handles a single interaction:  \n",
        "     - Stores the user’s input.  \n",
        "     - Adds conversation history to the query for context.  \n",
        "     - Runs the **RAG pipeline** to retrieve docs and generate an answer.  \n",
        "     - Stores the assistant’s reply back in history.  \n",
        "   - Returns both the assistant’s answer and full pipeline details (sources, retrieved docs, context).  \n",
        "\n",
        "3. **Single-Turn Demo**  \n",
        "   - Runs `chatbot_turn(\"Who created Python and when?\")` to test the chatbot flow.  \n",
        "   - Prints the bot’s answer, retrieved document IDs, and detected sources.  \n",
        "\n",
        "4. **Interactive Loop**  \n",
        "   - Starts a live conversation where the user can continuously ask questions.  \n",
        "   - The chatbot responds until the user types `\"exit\"` or `\"quit\"`.  \n",
        "\n",
        " **Why this section matters:**  \n",
        "This step transforms the project from a **retrieval pipeline** into a fully usable **chat interface**.  \n",
        "The chatbot can now handle **dialogue continuity** (memory of past turns) while grounding its answers in retrieved knowledge, making it practical for real-world applications.  \n"
      ],
      "metadata": {
        "id": "hAkutvSDXZmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation memory setup\n",
        "\n",
        "conversation_history = []\n"
      ],
      "metadata": {
        "id": "qEEOy_6bXgNY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# chatbot_turn - one conversational turn using RAG\n",
        "\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "# Ensure conversation history exists (Cell 7.1 should create this, but safe-guard here)\n",
        "try:\n",
        "    conversation_history  # noqa: F821\n",
        "except NameError:\n",
        "    conversation_history = []  # each item: {\"role\": \"user\"|\"assistant\", \"content\": str}\n",
        "\n",
        "def _format_history_for_prompt(history, max_turns=6) -> str:\n",
        "    \"\"\"\n",
        "    Format the last few turns into a short text block to include in the query.\n",
        "    Uses alternating User / Assistant lines to give context to the LLM.\n",
        "    \"\"\"\n",
        "    if not history:\n",
        "        return \"\"\n",
        "    # keep only the last max_turns entries (counting both user+assistant as separate turns)\n",
        "    snippet = history[-max_turns:]\n",
        "    lines = []\n",
        "    for h in snippet:\n",
        "        role = h.get(\"role\", \"user\")\n",
        "        content = h.get(\"content\", \"\").strip().replace(\"\\n\", \" \")\n",
        "        prefix = \"User:\" if role == \"user\" else \"Assistant:\"\n",
        "        lines.append(f\"{prefix} {content}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def chatbot_turn(user_input: str,\n",
        "                 top_k: int = 3,\n",
        "                 per_doc_chars: int = 500,\n",
        "                 max_history_turns: int = 6,\n",
        "                 debug: bool = False) -> Tuple[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Handle one chat turn:\n",
        "      - Add user turn to conversation_history\n",
        "      - Build a question that includes a short history snippet\n",
        "      - Run the RAG pipeline\n",
        "      - Append assistant reply to conversation_history\n",
        "    Returns:\n",
        "      (assistant_text, pipeline_result_dict)\n",
        "    \"\"\"\n",
        "    global conversation_history\n",
        "\n",
        "    # 1) store user turn\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # 2) build a short history snippet to include in retrieval/prompt\n",
        "    history_snippet = _format_history_for_prompt(conversation_history, max_turns=max_history_turns)\n",
        "\n",
        "    # Compose the effective question to send to RAG.\n",
        "    # We keep the final natural question clear for embedding/retrieval, but include the snippet so LLM sees the dialog.\n",
        "    if history_snippet:\n",
        "        # Make sure not to create an enormous query — this is just a short summary of recent turns.\n",
        "        effective_question = f\"Conversation History:\\n{history_snippet}\\n\\nUser question:\\n{user_input}\"\n",
        "    else:\n",
        "        effective_question = user_input\n",
        "\n",
        "    if debug:\n",
        "        print(\"=== chatbot_turn debug ===\")\n",
        "        print(\"Effective question sent to RAG (preview):\", effective_question[:400])\n",
        "\n",
        "    # 3) Run the RAG pipeline (retrieve -> prompt -> LLM)\n",
        "    pipeline_result = run_rag_pipeline(\n",
        "        effective_question,\n",
        "        top_k=top_k,\n",
        "        per_doc_chars=per_doc_chars,\n",
        "        debug=debug\n",
        "    )\n",
        "\n",
        "    assistant_text = pipeline_result.get(\"answer\", \"\").strip()\n",
        "\n",
        "    # 4) Append assistant reply to history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_text})\n",
        "\n",
        "    if debug:\n",
        "        print(\"Assistant preview:\", assistant_text[:200])\n",
        "        print(\"Sources returned:\", pipeline_result.get(\"sources\"))\n",
        "\n",
        "    return assistant_text, pipeline_result\n"
      ],
      "metadata": {
        "id": "CpfwIcjvX_bJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run one turn\n",
        "reply, info = chatbot_turn(\"Who created Python and when?\", top_k=3, debug=True)\n",
        "print(\"Bot:\", reply)\n",
        "# view retrieved docs & sources:\n",
        "print(\"Retrieved doc ids:\", [d[\"_id\"] for d in info[\"docs\"]])\n",
        "print(\"Detected sources in answer:\", info[\"sources\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOUGzo5DYWBS",
        "outputId": "74d2833a-0ad8-4554-c367-18d6e93e8b6e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== chatbot_turn debug ===\n",
            "Effective question sent to RAG (preview): Conversation History:\n",
            "User: Who created Python and when?\n",
            "\n",
            "User question:\n",
            "Who created Python and when?\n",
            "[retrieve] query embedded (len=384)\n",
            "[retrieve] Atlas returned 3 docs\n",
            "[RAG] Retrieved 3 docs. ids: [0, 3, 2]\n",
            "[RAG] Context preview: [source:doc_1] Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasizes code readability with its notable use of significant whitespace.     Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.     The language is widely used for web developm\n",
            "Calling Azure with messages (truncated):\n",
            "SYSTEM: You are an assistant that answers user questions using ONLY the provided CONTEXT. Cite sources inline using [source:doc_X]. If the answer cannot be found in the context, say 'I don't know' and do not hallucinate.\n",
            "USER: CONTEXT:\n",
            "[source:doc_1] Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasizes code readability with its notable use of significant whitespace.     Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.     The language is widely used for web development, data analysis, artificial intelligence, and automation.     Popular Python frameworks include Django for web d\n",
            "\n",
            "[source:doc_4] Discord Bot Development:     Discord bots are applications that can interact with Discord servers automatically.     They can respond to messages, moderate chat, play music, and perform various automated tasks.     Discord bots are built using Discord's API a\n",
            "Assistant preview: Python was created by Guido van Rossum in 1991 [source:doc_1].\n",
            "Sources returned: ['doc_1']\n",
            "Bot: Python was created by Guido van Rossum in 1991 [source:doc_1].\n",
            "Retrieved doc ids: [0, 3, 2]\n",
            "Detected sources in answer: ['doc_1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive loop\n",
        "\n",
        "while True:\n",
        "    q = input(\"You: \")\n",
        "    if q.lower() in [\"exit\", \"quit\"]: break\n",
        "    answer = chatbot_turn(q)\n",
        "    print(\"Bot:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "klmQHpT0YY0g",
        "outputId": "b5264b25-a07a-4f9e-f98b-d95e4f46564f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hey \n",
            "Bot: (\"I don't know.\", {'question': 'Conversation History:\\nUser: Who created Python and when?\\nAssistant: Python was created by Guido van Rossum in 1991 [source:doc_1].\\nUser: hey\\n\\nUser question:\\nhey ', 'answer': \"I don't know.\", 'docs': [{'_id': 0, 'text': 'Python Programming Language Overview:\\n    Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.\\n    It emphasizes code readability with its notable use of significant whitespace.\\n    Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\\n    The language is widely used for web development, data analysis, artificial intelligence, and automation.\\n    Popular Python frameworks include Django for web development, NumPy for scientific computing,\\n    and TensorFlow for machine learning applications.', 'source': 'doc_1', 'score': 0.742900550365448}, {'_id': 3, 'text': \"Discord Bot Development:\\n    Discord bots are applications that can interact with Discord servers automatically.\\n    They can respond to messages, moderate chat, play music, and perform various automated tasks.\\n    Discord bots are built using Discord's API and can be developed in multiple programming languages.\\n    Python developers often use the discord.py library to create bots with features like slash commands.\\n    Bots require proper authentication using bot tokens and must be invited to servers with appropriate permissions.\\n    Common bot features include welcome messages, role management, music playback, and custom commands.\", 'source': 'doc_4', 'score': 0.710843563079834}, {'_id': 2, 'text': 'Web Development Technologies:\\n    Web development involves creating websites and web applications using various technologies.\\n    Frontend development focuses on user interfaces using HTML, CSS, and JavaScript.\\n    Backend development handles server-side logic, databases, and APIs using languages like Python, Java, or Node.js.\\n    REST APIs provide a way for different systems to communicate over HTTP using standard methods.\\n    Modern web frameworks like React, Vue.js, and Angular help build interactive user interfaces.\\n    Database systems like PostgreSQL, MongoDB, and Redis store and manage application data efficiently.', 'source': 'doc_3', 'score': 0.561567485332489}], 'sources': [], 'context': \"[source:doc_1] Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasizes code readability with its notable use of significant whitespace.     Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.     The language is widely used for web development, data analysis, artificial intelligence, and automation.     Popular Python frameworks include Django for web d\\n\\n[source:doc_4] Discord Bot Development:     Discord bots are applications that can interact with Discord servers automatically.     They can respond to messages, moderate chat, play music, and perform various automated tasks.     Discord bots are built using Discord's API and can be developed in multiple programming languages.     Python developers often use the discord.py library to create bots with features like slash commands.     Bots require proper authentication using bot tokens and must be invited to se\\n\\n[source:doc_3] Web Development Technologies:     Web development involves creating websites and web applications using various technologies.     Frontend development focuses on user interfaces using HTML, CSS, and JavaScript.     Backend development handles server-side logic, databases, and APIs using languages like Python, Java, or Node.js.     REST APIs provide a way for different systems to communicate over HTTP using standard methods.     Modern web frameworks like React, Vue.js, and Angular help build int\"})\n",
            "You: What is discord?\n",
            "Bot: ('Discord is a platform where Discord bots can interact with servers automatically [source:doc_4].', {'question': \"Conversation History:\\nUser: Who created Python and when?\\nAssistant: Python was created by Guido van Rossum in 1991 [source:doc_1].\\nUser: hey\\nAssistant: I don't know.\\nUser: What is discord?\\n\\nUser question:\\nWhat is discord?\", 'answer': 'Discord is a platform where Discord bots can interact with servers automatically [source:doc_4].', 'docs': [{'_id': 3, 'text': \"Discord Bot Development:\\n    Discord bots are applications that can interact with Discord servers automatically.\\n    They can respond to messages, moderate chat, play music, and perform various automated tasks.\\n    Discord bots are built using Discord's API and can be developed in multiple programming languages.\\n    Python developers often use the discord.py library to create bots with features like slash commands.\\n    Bots require proper authentication using bot tokens and must be invited to servers with appropriate permissions.\\n    Common bot features include welcome messages, role management, music playback, and custom commands.\", 'source': 'doc_4', 'score': 0.8185335993766785}, {'_id': 0, 'text': 'Python Programming Language Overview:\\n    Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.\\n    It emphasizes code readability with its notable use of significant whitespace.\\n    Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\\n    The language is widely used for web development, data analysis, artificial intelligence, and automation.\\n    Popular Python frameworks include Django for web development, NumPy for scientific computing,\\n    and TensorFlow for machine learning applications.', 'source': 'doc_1', 'score': 0.7317173480987549}, {'_id': 2, 'text': 'Web Development Technologies:\\n    Web development involves creating websites and web applications using various technologies.\\n    Frontend development focuses on user interfaces using HTML, CSS, and JavaScript.\\n    Backend development handles server-side logic, databases, and APIs using languages like Python, Java, or Node.js.\\n    REST APIs provide a way for different systems to communicate over HTTP using standard methods.\\n    Modern web frameworks like React, Vue.js, and Angular help build interactive user interfaces.\\n    Database systems like PostgreSQL, MongoDB, and Redis store and manage application data efficiently.', 'source': 'doc_3', 'score': 0.5831049680709839}], 'sources': ['doc_4'], 'context': \"[source:doc_4] Discord Bot Development:     Discord bots are applications that can interact with Discord servers automatically.     They can respond to messages, moderate chat, play music, and perform various automated tasks.     Discord bots are built using Discord's API and can be developed in multiple programming languages.     Python developers often use the discord.py library to create bots with features like slash commands.     Bots require proper authentication using bot tokens and must be invited to se\\n\\n[source:doc_1] Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasizes code readability with its notable use of significant whitespace.     Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.     The language is widely used for web development, data analysis, artificial intelligence, and automation.     Popular Python frameworks include Django for web d\\n\\n[source:doc_3] Web Development Technologies:     Web development involves creating websites and web applications using various technologies.     Frontend development focuses on user interfaces using HTML, CSS, and JavaScript.     Backend development handles server-side logic, databases, and APIs using languages like Python, Java, or Node.js.     REST APIs provide a way for different systems to communicate over HTTP using standard methods.     Modern web frameworks like React, Vue.js, and Angular help build int\"})\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3875411250.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: Testing and evaluation\n",
        "\n",
        "\n",
        "In this section, we validate the performance of the RAG chatbot by running different types of queries and checking if responses are accurate, relevant, and grounded in the knowledge base.  \n",
        "\n",
        "The evaluation process includes four parts:  \n",
        "\n",
        "1. **Smoke Tests (Simple Queries)**  \n",
        "   - Run a set of straightforward questions such as *“Who created Python?”* or *“What is machine learning?”*.  \n",
        "   - Verify that the chatbot provides correct answers with proper source citations.  \n",
        "\n",
        "2. **Edge Case Queries**  \n",
        "   - Test queries outside the knowledge base (e.g., *“Who is Elon Musk?”*).  \n",
        "   - Check overlapping/multi-topic queries (e.g., *“Tell me about Python and Discord together”*).  \n",
        "   - Confirm that the chatbot either answers correctly from available context or says *“I don’t know”* rather than hallucinating.  \n",
        "\n",
        "3. **Atlas vs. Fallback Search Comparison**  \n",
        "   - Compare retrieval quality between **Atlas Vector Search** and manual cosine similarity fallback.  \n",
        "   - Ensures system resilience if Atlas is unavailable.  \n",
        "\n",
        "4. **Evaluation Helper Function**  \n",
        "   - `evaluate_queries()` automates testing against a set of queries with expected keywords.  \n",
        "   - Returns pass/fail results for each query, helping quantify accuracy.  \n",
        "\n",
        " **Relevance of this section:**  \n",
        "Evaluation ensures the RAG pipeline is **trustworthy, robust, and production-ready**.  \n",
        "By testing both normal and edge cases, as well as validating fallback mechanisms, we confirm that the chatbot behaves consistently in real-world scenarios.  \n"
      ],
      "metadata": {
        "id": "MHYSiOkEZRZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simple Queries Test (smoke test)\n",
        "\n",
        "\n",
        "test_queries = [\n",
        "    \"Who created Python?\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Tell me about Discord bots\"\n",
        "]\n",
        "\n",
        "print(\"🔎 Running simple smoke test queries...\\n\")\n",
        "for q in test_queries:\n",
        "    answer, info = chatbot_turn(q, top_k=3)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {answer}\\n\")\n",
        "    print(f\"Sources: {info['sources']}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ62VNGJZWru",
        "outputId": "9f8e36f9-8f35-45cc-eff5-5f251131c050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Running simple smoke test queries...\n",
            "\n",
            "Q: Who created Python?\n",
            "A: Python was created by Guido van Rossum in 1991 [source:doc_1].\n",
            "\n",
            "Sources: ['doc_1']\n",
            "------------------------------------------------------------\n",
            "Q: What is machine learning?\n",
            "A: Machine learning is a subset of artificial intelligence that enables computers to learn from data automatically [source:doc_2].\n",
            "\n",
            "Sources: ['doc_2']\n",
            "------------------------------------------------------------\n",
            "Q: Tell me about Discord bots\n",
            "A: Discord bots are applications that can interact with Discord servers automatically, respond to messages, moderate chat, play music, and perform various automated tasks. They are built using Discord's API and can be developed in multiple programming languages like Python with the discord.py library for features like slash commands [source:doc_4].\n",
            "\n",
            "Sources: ['doc_4']\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge Cases\n",
        "\n",
        "edge_queries = [\n",
        "    \"Who is Elon Musk?\",         # out of scope\n",
        "    \"Explain quantum physics\",   # not in knowledge base\n",
        "    \"Tell me about Python and Discord together\"  # overlapping topics\n",
        "]\n",
        "\n",
        "print(\"⚠️ Testing edge cases...\\n\")\n",
        "for q in edge_queries:\n",
        "    answer, info = chatbot_turn(q, top_k=3)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {answer}\\n\")\n",
        "    print(f\"Sources: {info['sources']}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBAO0LM8Zocs",
        "outputId": "aaebc9e3-a1e3-45e3-ff58-474cafd9f3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Testing edge cases...\n",
            "\n",
            "Q: Who is Elon Musk?\n",
            "A: I don't know.\n",
            "\n",
            "Sources: []\n",
            "------------------------------------------------------------\n",
            "Q: Explain quantum physics\n",
            "A: I don't know.\n",
            "\n",
            "Sources: []\n",
            "------------------------------------------------------------\n",
            "Q: Tell me about Python and Discord together\n",
            "A: Python is commonly used for developing Discord bots, with Python developers often utilizing the discord.py library to create bots with features like slash commands [source:doc_4].\n",
            "\n",
            "Sources: ['doc_4']\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Atlas vs Fallback\n",
        "\n",
        "\n",
        "def compare_search(query, top_k=3):\n",
        "    print(f\"\\n🔎 Query: {query}\")\n",
        "\n",
        "    atlas_res = mongodb_vector_search(query, top_k=top_k, debug=False)\n",
        "    if atlas_res:\n",
        "        print(\"\\n✅ Atlas Vector Search Results:\")\n",
        "        for i, r in enumerate(atlas_res, 1):\n",
        "            print(f\" {i}. score={r.get('score'):.4f} | source={r.get('source')}\")\n",
        "            print(\"    preview:\", r.get(\"text\",\"\")[:150].replace(\"\\n\",\" \"))\n",
        "    else:\n",
        "        print(\"\\n⚠️ Atlas returned no results.\")\n",
        "\n",
        "    fb_res = fallback_search(query, top_k=top_k)\n",
        "    print(\"\\n🟡 Fallback Cosine Similarity Results:\")\n",
        "    for i, r in enumerate(fb_res, 1):\n",
        "        print(f\" {i}. score={r['score']:.4f} | source={r['source']}\")\n",
        "        print(\"    preview:\", r['text'][:150].replace(\"\\n\",\" \"))\n",
        "\n",
        "# Example test\n",
        "compare_search(\"What is Python programming?\", top_k=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x80RzCdoZ5ZM",
        "outputId": "d874d090-a2cd-43b3-f19f-1ad21ad7cf17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Query: What is Python programming?\n",
            "\n",
            "✅ Atlas Vector Search Results:\n",
            " 1. score=0.9127 | source=doc_1\n",
            "    preview: Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasi\n",
            " 2. score=0.7424 | source=doc_4\n",
            "    preview: Discord Bot Development:     Discord bots are applications that can interact with Discord servers automatically.     They can respond to messages, mod\n",
            " 3. score=0.6612 | source=doc_3\n",
            "    preview: Web Development Technologies:     Web development involves creating websites and web applications using various technologies.     Frontend development\n",
            "\n",
            "🟡 Fallback Cosine Similarity Results:\n",
            " 1. score=0.8254 | source=doc_1\n",
            "    preview: Python Programming Language Overview:     Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.     It emphasi\n",
            " 2. score=0.4848 | source=doc_4\n",
            "    preview: Discord Bot Development:     Discord bots are applications that can interact with Discord servers automatically.     They can respond to messages, mod\n",
            " 3. score=0.3224 | source=doc_3\n",
            "    preview: Web Development Technologies:     Web development involves creating websites and web applications using various technologies.     Frontend development\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Evaluation Helper\n",
        "\n",
        "\n",
        "def evaluate_queries(test_cases, top_k=3):\n",
        "    \"\"\"\n",
        "    Runs multiple queries and checks if answer contains expected keywords.\n",
        "    test_cases: list of dicts {query: str, expected_keywords: [list of str]}\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for case in test_cases:\n",
        "        q = case[\"query\"]\n",
        "        expected = case.get(\"expected_keywords\", [])\n",
        "        ans, info = chatbot_turn(q, top_k=top_k)\n",
        "\n",
        "        passed = all(kw.lower() in ans.lower() for kw in expected)\n",
        "        results.append({\"query\": q, \"expected\": expected, \"answer\": ans, \"pass\": passed})\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n📊 Evaluation Results:\")\n",
        "    for r in results:\n",
        "        status = \"✅ PASS\" if r[\"pass\"] else \"❌ FAIL\"\n",
        "        print(f\"Q: {r['query']}\")\n",
        "        print(f\"Expected keywords: {r['expected']}\")\n",
        "        print(f\"A: {r['answer'][:200]}...\")\n",
        "        print(\"Result:\", status)\n",
        "        print(\"-\" * 60)\n",
        "    return results\n",
        "\n",
        "# Example evaluation set\n",
        "test_cases = [\n",
        "    {\"query\": \"Who created Python?\", \"expected_keywords\": [\"Guido\", \"1991\"]},\n",
        "    {\"query\": \"What is machine learning?\", \"expected_keywords\": [\"data\", \"learn\"]},\n",
        "    {\"query\": \"What are Discord bots?\", \"expected_keywords\": [\"Discord\", \"automated\"]}\n",
        "]\n",
        "\n",
        "evaluate_queries(test_cases)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0khFdA1aZ6AS",
        "outputId": "7e0a31bb-99f7-4fc8-bd76-9006cffc8653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "Q: Who created Python?\n",
            "Expected keywords: ['Guido', '1991']\n",
            "A: Python was created by Guido van Rossum in 1991 [source:doc_1]....\n",
            "Result: ✅ PASS\n",
            "------------------------------------------------------------\n",
            "Q: What is machine learning?\n",
            "Expected keywords: ['data', 'learn']\n",
            "A: Machine learning is a subset of artificial intelligence that enables computers to learn from data automatically. It includes supervised learning, unsupervised learning, and reinforcement learning [sou...\n",
            "Result: ✅ PASS\n",
            "------------------------------------------------------------\n",
            "Q: What are Discord bots?\n",
            "Expected keywords: ['Discord', 'automated']\n",
            "A: Discord bots are applications that can interact with Discord servers automatically, respond to messages, moderate chat, play music, and perform various automated tasks [source:doc_4]....\n",
            "Result: ✅ PASS\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'query': 'Who created Python?',\n",
              "  'expected': ['Guido', '1991'],\n",
              "  'answer': 'Python was created by Guido van Rossum in 1991 [source:doc_1].',\n",
              "  'pass': True},\n",
              " {'query': 'What is machine learning?',\n",
              "  'expected': ['data', 'learn'],\n",
              "  'answer': 'Machine learning is a subset of artificial intelligence that enables computers to learn from data automatically. It includes supervised learning, unsupervised learning, and reinforcement learning [source:doc_2].',\n",
              "  'pass': True},\n",
              " {'query': 'What are Discord bots?',\n",
              "  'expected': ['Discord', 'automated'],\n",
              "  'answer': 'Discord bots are applications that can interact with Discord servers automatically, respond to messages, moderate chat, play music, and perform various automated tasks [source:doc_4].',\n",
              "  'pass': True}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}
